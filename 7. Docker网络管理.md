## Docker网络管理

１．Docker网络基础

在一台未经特殊网络配置的Ubuntu机器上安装玩Docker之后，在宿主机上通过使用ifconfig命令可以看到多了一块名为docker0的网卡，假设为IP为172.17.0.1/16，有了这样一块网卡，宿主机也会在内核路由表上添加一条到达相应网络的静态路由，可以通过route -n命令查看．

```
zjj@zjj-ThinkCentre-M8600t-D065:~$ route -n
Kernel IP routing table
Destination     Gateway         Genmask         Flags Metric Ref    Use Iface
0.0.0.0         192.168.1.1     0.0.0.0         UG    100    0        0 eno1
169.254.0.0     0.0.0.0         255.255.0.0     U     1000   0        0 eno1
172.17.0.0      0.0.0.0         255.255.0.0     U     0      0        0 docker0
192.168.1.0     0.0.0.0         255.255.255.0   U     100    0        0 eno1

```

所有目的IP地址为127.17.0.0的数据包从docker0网卡发出

在执行下述命令创建一个Docker容器时：

```
sudo docker run -it --name container -h jj ubuntu /bin/bash
```

在container容器中可以看到它有两块网卡lo和eth0,lo设备为本机的回环网卡；eth0即为容器与外界通信的网卡，eth0的IP为172.17.0.２，和宿主机上的网桥docker0在同一个网段．container的默认网关是宿主机的docker0网卡

这时查看宿主机的网络设备会发现有一块以＇veth＇开头的网卡，如vethdb2b886，这块网卡为veth设备，而veth pair总是成对出现，通常用来链接两个netword namespace，而拧一个应该是Docker容器container中的eth0．docker0其实是网桥．以veth pair连接各容器，容器中的数据通过docker0网桥转发到eth0网卡上．

在linux中可以使用brctl命令查看和管理网桥（需要安装bridge-utils软件包）,如下：

```
zjj@zjj-ThinkCentre-M8600t-D065:~$ sudo brctl show
bridge name	bridge id		STP enabled	interfaces
docker0		8000.024255b47202	no		vethdb2b886
```

docker0网桥是在Docker Daemon启动时自动创建的，地址默认为172.17.0.1/16，之后创建的容器都会在docker0子网的范围内选取一个未被占用的IP使用，并连接到docker0网桥上．

２．iptables规则

Docker安装完成后，将默认在宿主机系统上增加一些iptables规则，以用于Docker容器和容器之间以及和外界的通信

３．Docker容器的DNS和主机名

同一个镜像可以启动多个Docker容器，通过查看，他们的主机名并不一样，也即是说主机名并非被写入镜像中．这样能解决主机名的问题，同时也能让DNS及时更新．

４．Docker容器的４中网络模式

在docker run创建Docker容器时，可以用--net选项指定容器得到网路模式，Docker有一下四种网络模式：

1. bridge模式

   使用--net=bridge指定，为Docker设置默认设置，这种模式就是创建出来的docker容器连接到Docker网桥上，我们之前创建的容器都是这种模式，其与外部通信使用NAT协议，在这种模式下，Docker初始化Docker容器网络的步骤如下：

   （１）创建一个虚拟网卡(veth pair),

   （２）赋予其中一块网卡一个类似于vethdb2b886的名字，将其留在root network namespace中，并绑定到Docker网桥上

   （３）将另一快网卡新放入新创建的network namespace中（Docker容器中），命名为eth0.

   （４）从Docker网桥的子网中选取一个未使用的IP分配给eth0，并为Docker容器设置默认路由，默认网关为Docker网桥．

2. host模式

   使用--net=host指定，这种模式Docker server将不为Docker容器创建网络协议栈，即不会创建独立的network namespace，　那么上面的bridge步骤将不会进行．Docker容器中的进程处于宿主机的网络环境中，相当于Docker容器和宿主机公用一个network namespace，使用宿主机的网卡，IO和端口信息．但是容器的文件系统，进程列表等还是和宿主机隔离．

   host模式很好的解决了容器与外界通信的地址转换问题，可以直接使用宿主机的IP进行通信，但也降低了隔离性，同时会引起网络资源的竞争和冲突．

3. container模式

   此模式和host模式类似，指定新创建的容器与已经存在的某个容器共享同一个network　namespace．都是共享network namespace,区别就在于host与宿主机共享，而container模式与某一个存在的容器共享．新创建的容器不会创建自己的虚拟网卡，也不配置IP，只会与一个指定的容器共享IP和端口范围等．同样，两个容器除了网络方面，其它都是隔离的．在这种模式下，两个容器的进程可以通过lo回环网卡设备通信，增加了容器间通信的便利性和效率．

4. none模式

   这种模式下，Docker容器拥有自己的nework namespace，但是，并不为Docker 容器进行任何网络配置．这种模式下用户需要为Docker容器添加网卡，配置IP等，给了容器最大的自由度来自定义容器的网络环境．

   ​


５．玩转Linux Network namespace

1.  使用ip netns命令操作network namespace:

   ip netns是用来操作network namespace的指令，具体方法如下：

   ```
   # 创建一个名为nstest的network namespace
   $ sudo ip netns add nstest
   ```

   列出系统中已存在的network namespace:

   ```
   $sudo ip netns list
   ```

   删除一个network namespace:

   ```
   $ sudo ip netns delete nstest
   ```

   在network namespace中执行一条指令：

   ```
   # 命令格式
   sudo ip netns exec <network namespace name> <command>
   # 如显示nstest namespace中的网卡信息
   $ sudo ip netns exec nstest ip addr
   ```

   在network namespace中启动一个shell是更加直接的做法：

   ```
   # 命令格式
   sudo ip netns exec <network namespace name> bash
   ```

   ​

2. 使用ip命令为network namespace配置网卡

   在使用ip netns add命令添加一个network namespace后，就拥有了一个独立的网络空间，可以按照需求配置该网络空间，如添加网卡，配置IP和设置路由规则等．

   当使用ip命令创建一个network namespace时，会默认创建一个回环设备（loopback interface:lo）．该设备默认不会启动，手动启动命令如下：

   ```
   $ sudo ip netns exec nstest ip link set dev lo up
   ```

   在主机上创建两个虚拟网卡veth-a 和veth-b：

   ```
   $ sudo ip link add veth-a type veth peer name veth-b
   ```

   将veth-b添加到nstest这个network namespace中，将veth-a留在主机上

   ```
   $ sudo ip link set veth-b netns nstest
   ```

   现在nstest就有两张网卡：lo和veth-b，如下：

   ```
   $ sudo ip netns exec nstest ip addr
   1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
       link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
       inet 127.0.0.1/8 scope host lo
          valid_lft forever preferred_lft forever
       inet6 ::1/128 scope host 
          valid_lft forever preferred_lft forever
   45: veth-b@if46: <BROADCAST,MULTICAST> mtu 1500 qdisc noop state DOWN group default qlen 1000
       link/ether fa:c3:a7:3e:ca:e7 brd ff:ff:ff:ff:ff:ff link-netnsid 0

   ```

   现在可以为网卡分配IP并启动网卡：

   ```
   #　在主机为veth-a配置IP并启动
   $ sudo ip addr add 10.0.0.1/24 dev veth-a
   $ sudo ip link set dev veth-a up

   #　为nstest中的veth-b配置IP并启动
   $ sudo ip netns exec nstest ip addr add 10.0.0.2/24 dev veth-b
   $ sudo ip netns exec nstest ip link set dev veth-b up
   ```

   给两张网卡配置IP后，会在各自的network namespace中生成一条路由，用ip route或route -a命令查看：

   ```
   #　在主机中
   $ sudo ip route
   default via 192.168.1.1 dev eno1  proto static  metric 100 
   10.0.0.0/24 dev veth-a  proto kernel  scope link  src 10.0.0.1 

   # 在nstest network namespace中
   $ sudo ip netns exec nstest ip route
   10.0.0.0/24 dev veth-b  proto kernel  scope link  src 10.0.0.2 
   ```

   这两条路由表意义是：目的地址为10.0.0.0/24网络的IP包分别从veth-a和veth-b发出．

   现在nstest之歌network namespace已经有了自己的网卡，IP地址，路由表等信息，俨然称为了一台小型的＂虚拟机＂．下面测试其连通性：

   ```
   # 从主机的veth-a 网卡ping nstest network namespace 的veth-b网卡
   $ ping 10.0.0.2
   PING 10.0.0.2 (10.0.0.2) 56(84) bytes of data.
   64 bytes from 10.0.0.2: icmp_seq=1 ttl=64 time=0.042 ms
   ...

   ＃　从netns network namespace的veth-b网卡ping 主机的veth-a网卡：
   $ sudo ip netns exec nstest ping 10.0.0.1
   [sudo] password for zjj: 
   PING 10.0.0.1 (10.0.0.1) 56(84) bytes of data.
   64 bytes from 10.0.0.1: icmp_seq=1 ttl=64 time=0.028 ms
   ...
   ```

   ​

3. 将两个network namespace连接起来

   在单机上，创建许多相互隔离的network namespace，然后通过网卡，网桥等虚拟设备将他们连接起来，组成想要的复杂拓扑网络．

   将两个network namespace通过veth pair设备连接起来，具体过程如下：

   ```
   # 创建两个network namespace ns1, ns2
   $ sudo ip netns add ns1
   $ sudo ip netns add ns2
   #　创建veth pair设备veth_a, veth_b
   $ sudo ip link add veth_a type veth peer name veth_b
   #　将网卡分别放入两个namespace中
   $ sudo ip link set veth-a netns ns1
   $ sudo ip link set veth-b netns ns2
   # 启动两张网卡
   $ sudo ip netns exec ns1 ip link set dev veth_a up
   $ sudo ip netns exec ns2 ip link set dev veth_b up
   # 分配IP
   $ sudo ip netns exec ns1 ip addr add 10.0.0.1/24 dev veth_a
   $ sudo ip netns exec ns2 ip addr add 10.0.0.2/24 dev veth_b
   ＃ 验证连通性
   $ sudo ip netns exec ns1 ping 10.0.0.2
   ```

   通过veth pair 设备连接起来的两个network namespace就好像直接通过网线连接的两台机器，拓扑图如下：

   ![img](https://github.com/WhiteGoing/picture/commit/857e912ebb7e3d11f91ecd71b30d596b81faba16)

   如果需要有更多network namespace需要连接，必须引入虚拟网桥

4. ​








